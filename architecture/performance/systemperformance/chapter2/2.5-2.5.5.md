# 🎯 **System Performance Analysis Methodologies: A Complete Guide**

## 🌟 **Classification of methodologies**

### 🔍 **Types of analysis approaches**
| **Type of analysis**  | **Features**                       | **Examples**         |
| --------------------- | ---------------------------------- | -------------------- |
| **Observant**         | Studying metrics and logs          | USE, RED, Drill-Down |
| **Experimental**      | Parameter modification and testing | Micro-benchmarking   |
| **Hypothetical**      | Hypothesis testing                 | Scientific method    |
| **Capacity planning** | Load and resource forecasting      | Queueing Theory      |

## 🚫 **Anti-methodology (how NOT to do it)**

### 1. **The "street lamp" method (2.5.1)**
- **Problem**: Analyzing only familiar areas
- **Example**: Using `top` instead of `perf` for CPU diagnostics
- **How to fix**: Apply the USE method for full coverage

### 2. **Random changes (2.5.2)**
- **Risk**: Degradation in production
- **Case**: Changing `vm.swappiness` without OOM killer monitoring

### 3. **Shifting the blame (2.5.3)**
- **Typical phrase**: "This is definitely a network!"
- **Solution**: Require data (e.g. `mtr` reports)

## ✅ **Effective methodologies**

### 🔥 **TOP 5 required techniques**

1. **Problem statement (2.5.5)**
```markdown
1. What are the symptoms?
   2. When was the last time it worked fine?
   3. What has changed?
   4. How to measure the problem?
   5. Who else is affected?
   ```

2. **USE Method (2.5.9)**
   - **Utilization**: `mpstat -P ALL 1`
   - **Saturation**: `vmstat 1' (column `r`)
   - **Errors**: `dmesg -T | grep -i error`

3. **RED Method (2.5.10)**
   ```bash
   # Rate (RPS):
   curl -s http://localhost/metrics | grep http_requests_total

   # Errors:
   cat /var/log/nginx/error.log | wc -l

   # Duration:
   p99_latency=$(curl -s http://localhost/metrics | grep 'http_request_duration_seconds{quantile="0.99"}')
   ```

4. **Delay Analysis (2.5.13)**
```bash
   # Linux:
   perf trace -p $(pgrep app) -e syscalls:sys_enter_*

   # MySQL:
   SELECT * FROM performance_schema.events_waits_history_long;
   ```

5. **Method R (2.5.14)**
   - **Record**: Trace recording
   - **Report**: Critical Path Analysis
   - **Replay**: Hypothesis verification

## 🛠️ **Specialized techniques**

### **For CPU (Chapter 6)**
- **Cycle analysis (6.5.5)**: `perf stat-e cycles,instructions,cache references`
- **CPU binding (6.5.9)**: `taskset -c 0.1 ./app`

### **For memory (Chapter 7)**
- **Leak detection (7.4.6)**: `valgrind --leak-check=yes ./app`
- **Memory compression (7.4.10)**: `echo 1 > /proc/sys/vm/compact_memory`

### **For the Network (Chapter 10)**
- **TCP Analysis (10.5.7)**: `ss -tin`
- **Packet sniffing (10.5.6)**: `tcpdump -i eth0 -w capture.pcap`

## 📈 **Advanced techniques**

### **Queue Theory (2.6.5)**
The formula for estimating the waiting time in M/M/1 is:
``
W = λ / (μ*(μ-λ))
``
Where:
- λ is the intensity of the incoming flow
- µ — processing intensity

### **Micro-benchmarking (2.5.19)**
IOPS test example:
```bash
fio --name=randread --ioengine=libaio --rw=randread --bs=4k --numjobs=4 --size=1G --runtime=60 --time_based
```

## 💡 **Key principles**

1. **Hierarchy of application**:
```mermaid
   graph TD
       A[Problem Statement] --> B[USE/RED]
       B --> C[Drill-Down]
       C --> D[Latency Analysis]
   ```

2. **Golden Rules**:
- Always measure before/after changes
   - Document hypotheses and results
- Start at the top level (application), then move on to the system

3. **Expert's Toolkit**:
```markdown
   - Monitoring: `bpftrace', `perf`, `eBPF'
- Profiling: `pprof`, `VTune`
   - Tracing: `strace', `ftrace`
   ```
