### **2.5.1 Streetlight Anti-Method**  
**Description:**  
This anti-method describes a common mistake when research focuses on easily accessible data (like a street lamp illuminating only a specific area), ignoring more complex but important aspects of the problem.  
**Example:** Using standard metrics instead of deep bottleneck analysis.  

---

### **2.5.2 Random Change Anti-Method**  
**Description:**  
Attempts to improve performance by making random changes without understanding their impact. It often leads to a worsening of the situation.  
**Example:** Adjust system parameters at random without measuring the results.  

---

### **2.5.3 Blame-Someone-Else Anti-Method**  
**Description:**  
Shifting responsibility for problems to other system components or teams without evidence.  
**Example:** Blaming the network for the slow operation of the application without analyzing its download.  

---

### **2.5.4 Ad Hoc Checklist Method**  
**Description:**  
Using ready-made checklists to adjust performance without taking into account the specifics of the system.  
**Example:** The application of universal recommendations for all servers.  

---

### **2.5.5 Problem Statement**  
**Description:**  
A formal definition of the problem, including symptoms, conditions of occurrence, and expected results.  
**Example:** "Database queries run for longer than 2 seconds under high load."  

---

### **2.5.6 Scientific Method**  
**Description:**  
Application of the scientific approach: hypothesis → experiment → analysis → conclusions.  
**Example:** The assumption that the cache reduces latency, and testing this in practice.  

---

### **2.5.7 Diagnosis Cycle**  
**Description:**  
A cyclical process of identifying and eliminating problems: data collection → analysis → correction → repetition.  
**Example:** Gradual clarification of the reason for high CPU usage.  

---

### **2.5.8 Tools Method**  
**Description:**  
Using specialized tools to collect and analyze performance data.  
**Example:** `perf`, `strace`, `vmstat`.  

---

### **2.5.9 The USE Method**  
**Description:**  
A performance analysis method that focuses on three aspects: Utilization, Saturation, and Errors.  
**Example:** Checking disk loading, request queue, and the number of I/O errors.  

---

### **2.5.10 The RED Method**  
**Description:**  
Analysis of three key metrics: Rate (frequency of events), Errors (errors), Duration (duration).  
**Example:** Monitoring the number of HTTP requests, 500 errors, and server response time.  

---

### **2.5.11 Workload Characterization**  
**Description:**  
Studying the characteristics of the load on the system: intensity, distribution, patterns.  
**Example:** Analysis of the peak time of users accessing the web application.  

---

### **2.5.12 Drill-Down Analysis**  
**Description:**  
A gradual deepening into the problem from the general to the particular.  
**Example:** From the total CPU load to the analysis of individual processes and threads.  

---

### **2.5.13 Latency Analysis**  
**Description:**  
Investigation of system delays and their sources.  
**Example:** Measurement of the database response time to a query.  

---

### **2.5.14 Method R**  
**Description:**  
A method for optimizing SQL queries, including analysis of execution time and resources.  
**Example:** Identification of "heavy" queries through "EXPLAIN ANALYSIS".  

---

### **2.5.15 Event Tracing**  
**Description:**  
Record and analyze events in the system to diagnose problems.  
**Example:** Using `ftrace` or `DTrace` to track system calls.  

---

### **2.5.16 Baseline Statistics**  
**Description:**  
Create "benchmark" performance metrics for comparison.  
**Example:** Measuring the system response time in normal mode.  

---

### **2.5.17 Static Performance Tuning**  
**Description:**  
Setting up the system based on pre-known recommendations.  
**Example:** Optimization of buffer size in DBMS.  

---

### **2.5.18 Cache Tuning**  
**Description:**  
Optimize cache usage to reduce latency.  
**Example:** Configure caching of web pages or database queries.  

---

### **2.5.19 Micro-Benchmarking**  
**Description:**  
Testing individual system components to evaluate their performance.  
**Example:** Measurement of the disk read/write speed.  

---

### **2.5.20 Performance Mantras**  
**Description:**  
Key principles and mantras for effective optimization, for example:  
- "Measure, don't assume."  
- "Optimize bottlenecks, not everything."  

---